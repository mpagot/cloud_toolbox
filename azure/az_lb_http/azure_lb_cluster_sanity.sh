# Create in Azure:
# 2 VM 
# 1 public IP
# 1 LB to dynamically assign the PublicIP to the 3 VM
# each VM has a simple web server installed (by cloud-init script)
# the VMs are part of a pacemaker cluster
# the cluster has a azure-lb RA
# the LB health probe is pointed to the port exposed by RA

. ./utils.sh

MY_PUBIP_ADDR="$(get_pub_ip)"

test_step "deployment"
az group list --query "[?name=='${MY_GROUP}'].name" -o tsv | wc -l | grep 1 || test_die "Resource group"
az vm list -g "${MY_GROUP}" --query "[?name=='${MY_BASTION}'].name" -o tsv | wc -l | grep 1 || test_die "Bastion"
az vm list -g "${MY_GROUP}" --query "[?name!='${MY_BASTION}'].name" -o tsv | wc -l | grep $MY_NUM || test_die "Nodes is not ${MY_NUM}"

for NUM in $(seq $MY_NUM); do
  this_vm="${MYNAME}-vm-0${NUM}"
  echo "###########################################################################"
  echo "# -------> this_vm:${this_vm}"

  test_step "[${this_vm}] vm get-instance-view"
  az vm get-instance-view --name "${this_vm}" --resource-group "${MY_GROUP}" --query "instanceView.statuses[1].[code,displayStatus]" -o tsv | grep -c running | grep 2 || test_die "VM ${this_vm} is not running"
  test_step "[${this_vm}] whoami"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' whoami' | grep $MY_USERNAME || test_die "Remote user is not ${MY_USERNAME}"
  test_step "[${this_vm}] sudo whoami"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo whoami' | grep root || test_die "Remote user with sudo is not root"
  test_step "[${this_vm}] ls .ssh"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' ls -lai ~/.ssh' | grep -v '.pub' | grep -c 'id_rsa' | grep 1 || test_die "${MY_USERNAME} on node${NUM} has one private key needed to talk to the other node"
  # disabled as it does not pass on vm2
  #ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' cat ~/.ssh/authorized_keys' | wc -l | grep 3 || test_die "${MY_USERNAME} on node${NUM} has 3 allowed keys in total, one is for the bastion"
  #ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' cat ~/.ssh/authorized_keys' | grep 'Temp internal cluster key for' | wc -l | grep 2 || test_die "${MY_USERNAME} on node${NUM} has 2 allowed keys for the inter-nodes communication"

  # ignore for the moment internal nodes keys for root as they self generated by crm and are not shared
  test_step "[${this_vm}] ls root .ssh"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo ls -lai /root/.ssh'
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo cat /root/.ssh/id_rsa.pub'
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo cat /root/.ssh/authorized_keys'
  #ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo ls -lai /root/.ssh' | grep -c 'id_rsa' | grep 0 || test_die "root on node${NUM} do not have private key"
  test_step "[${this_vm}] homes"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo find /home/ -type d -mindepth 1 -maxdepth 1' | wc -l | grep 1 || test_die "Only home for ${MY_USERNAME} is expected at this point"
  test_step "[${this_vm}] passwd"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo cat /etc/passwd' | grep -c -E "root|${MY_USERNAME}|hacluster" | grep 3 || test_die "node${NUM} has root, hacluster and ${MY_USERNAME}"
  test_step "[${this_vm}] private ip"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' ip a show eth0' | grep -E "inet .*192\.168\.1\.4${NUM}" || test_die "node${NUM} do not have private IP 192.168.1.4${NUM}"
  test_step "[${this_vm}] cluster"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo crm status' || test_die "node${NUM} fails calling crm status"

  test_step "[${this_vm}] connectivity"
  for OTHER_NUM in $(seq $MY_NUM); do
    if [ "$NUM" -eq "$OTHER_NUM" ]; then
        continue
    fi
    other_vm="${MYNAME}-vm-0${OTHER_NUM}"
    test_step "[${this_vm}]-->[${other_vm}] ping IP"
    ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' ping -c 10 192.168.1.4'"${OTHER_NUM}" || test_die "${this_vm} is not able to ping ${other_vm} at 192.168.1.4${OTHER_NUM}"
    test_step "[${this_vm}]-->[${other_vm}] ping name"
    ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' ping -c 10 '"${other_vm}" || test_die "${this_vm} is not able to ping ${other_vm} by name"
    test_step "[${this_vm}]-->[${other_vm}] ssh"
    ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' ssh '"${MY_USERNAME}@${other_vm}"' whoami'
    test_step "[${this_vm}]-->[${other_vm}] sudo ssh"
    ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo ssh '"${MY_USERNAME}@${other_vm}"' whoami'
  done

  test_step "[${this_vm}] load balancer"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' curl -H "Metadata:true" --noproxy "*" "http://169.254.169.254:80/metadata/loadbalancer?api-version=2020-10-01" | python3 -m json.tool'

  test_step "[${this_vm}] webserver"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' zypper se -i -s nginx' || test_die "${this_vm} does not have nginx installed"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo systemctl status nginx.service' || test_die "${this_vm} does not have nginx server running"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'curl http://'"${this_vm}" || test_die "${this_vm} does not have http web page reachable"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'curl http://'"${MY_FIP}" || test_die "${this_vm} does not have http web page reachable"

  test_step "[${this_vm}] diagnostic logs"
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' cat /etc/os-release'
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' uname -a'
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' zypper --version'
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo cat /var/log/cloud-init.log | grep -v DEBUG | grep -E "Command|Exit"'
  ssh -i $MYSSHKEY $MY_USERNAME@$MY_PUBIP_ADDR 'ssh '"${MY_USERNAME}@${this_vm}"' sudo journalctl -b | grep -E "cloud-init\[.*(Failed|Warning)"'
done